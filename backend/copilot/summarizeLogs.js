const { createChatCompletion } = require('./client');

// Modelos dispon√≠veis: openai/gpt-4o-mini, openai/gpt-4o, openai/gpt-4.1, etc.
// Formato: {publisher}/{model_name}
const DEFAULT_MODEL = process.env.GITHUB_MODEL || 'openai/gpt-4o-mini';
const MAX_LOGS = Number(process.env.COPILOT_MAX_LOGS) || 120;
const CHUNK_SIZE = Number(process.env.COPILOT_CHUNK_SIZE) || 40;

const SYSTEM_INSTRUCTIONS = [
    'Voc√™ √© um assistente que explica registros t√©cnicos de forma SIMPLES e HUMANA para pessoas sem conhecimento t√©cnico.',
    'Seja sempre favor√°vel ao app e √† empresa. Mostre confian√ßa e tranquilidade no funcionamento quando n√£o houver erros.',
    'Considere ruim APENAS quando houver erros nos registros. Se n√£o houver erro, est√° tudo certo.',
    'Quando houver erro, indique se √© algo realmente preocupante e, se for, diga que nossa equipe j√° est√° averiguando o ocorrido.',
    'Se houver erro e for algo ruim, sempre finalize dizendo que, em caso de d√∫vidas, a pessoa pode/deve entrar em contato com o suporte.',
    'Escreva como se estivesse conversando com um amigo que n√£o entende de tecnologia.',
    'NUNCA use termos t√©cnicos como "Runtime", "module", "exception", "invoke", "init". Traduza para linguagem comum.',
    'Em vez de "Runtime.ImportModuleError", diga "um arquivo necess√°rio n√£o foi encontrado".',
    'Em vez de "exception", diga "erro" ou "problema".',
    'Em vez de "invoke/init", diga "quando o sistema tentou executar".',
    'Seja breve, claro e tranquilizador. M√°ximo 500 caracteres.',
    'Se detectar problemas, explique o impacto pr√°tico (ex: "isso pode estar impedindo pedidos de serem criados").',
    'N√£o invente. Se n√£o souber, diga "n√£o foi poss√≠vel identificar".'
].join(' ');

const buildUserPrompt = ({
    integrationName,
    functionName,
    logs
}) => {
    const sanitizedLogs = logs.map(log => ({
        ts: log.timestamp,
        msg: log.simplifiedMessage || log.message,
        lvl: log.level
    }));

    return [
        `Sistema analisado: ${functionName || integrationName || 'Sistema'}`,
        `Quantidade de registros: ${sanitizedLogs.length}`,
        '',
        'Responda neste formato simples:',
        '',
        'üìä O que aconteceu:',
        '(explique em 2-3 frases simples o que os registros mostram)',
        '',
        '‚ö†Ô∏è Precisa de aten√ß√£o?',
        '(diga se h√° algo preocupante e o que pode significar na pr√°tica)',
        '',
        'üîç Quando ocorreu:',
        '(mencione os hor√°rios principais em formato leg√≠vel como "20/01 √†s 15:27")',
        '',
        'Registros para analisar:',
        JSON.stringify(sanitizedLogs)
    ].join('\n');
};

const buildChunkUserPrompt = ({ logs, chunkIndex, totalChunks }) => {
    const sanitizedLogs = logs.map(log => ({
        ts: log.timestamp,
        msg: log.simplifiedMessage || log.message,
        lvl: log.level
    }));

    return [
        `Lote ${chunkIndex}/${totalChunks} - Resuma em 2 bullets com timestamp:`,
        JSON.stringify(sanitizedLogs)
    ].join('\n');
};

const buildFinalUserPrompt = ({ chunkSummaries }) => [
    'Consolide os resumos parciais abaixo em um resumo final seguindo o formato:',
    '',
    'üìä O que aconteceu:',
    '(explique em 2-3 frases simples)',
    '',
    '‚ö†Ô∏è Precisa de aten√ß√£o?',
    '(diga se h√° algo preocupante)',
    '',
    'üîç Quando ocorreu:',
    '(hor√°rios principais)',
    '',
    'Resumos parciais:',
    chunkSummaries.join('\n\n')
].join('\n');

const summarizeLogs = async ({ logs, summary, integration }) => {
    if (!logs?.length) {
        return {
            summary: 'Nenhum log dispon√≠vel para o per√≠odo e filtro selecionados.'
        };
    }

    const model = DEFAULT_MODEL;
    const primaryLogs = logs.slice(0, MAX_LOGS);

    console.log(`[github-models] iniciando resumo com modelo ${model}, ${primaryLogs.length} logs`);

    try {
        // Se poucos logs, processa direto
        if (primaryLogs.length <= CHUNK_SIZE) {
            const response = await createChatCompletion({
                model,
                messages: [
                    { role: 'system', content: SYSTEM_INSTRUCTIONS },
                    {
                        role: 'user', content: buildUserPrompt({
                            integrationName: integration?.name,
                            functionName: integration?.function_name,
                            logs: primaryLogs
                        })
                    }
                ],
                maxTokens: 800
            });

            const content = response?.choices?.[0]?.message?.content?.trim();
            console.log(`[github-models] resumo gerado com sucesso`);

            return {
                summary: content || 'N√£o foi poss√≠vel gerar o resumo no momento.'
            };
        }

        // Muitos logs: processa em chunks
        const totalChunks = Math.ceil(primaryLogs.length / CHUNK_SIZE);
        const chunkSummaries = [];

        for (let i = 0; i < primaryLogs.length; i += CHUNK_SIZE) {
            const chunkIndex = Math.floor(i / CHUNK_SIZE) + 1;
            const chunkLogs = primaryLogs.slice(i, i + CHUNK_SIZE);

            console.log(`[github-models] processando chunk ${chunkIndex}/${totalChunks}`);

            try {
                const chunkResponse = await createChatCompletion({
                    model,
                    messages: [
                        { role: 'system', content: 'Resuma os logs de forma concisa em portugu√™s.' },
                        {
                            role: 'user', content: buildChunkUserPrompt({
                                logs: chunkLogs,
                                chunkIndex,
                                totalChunks
                            })
                        }
                    ],
                    maxTokens: 300
                });

                const chunkContent = chunkResponse?.choices?.[0]?.message?.content?.trim() || 'Sem eventos relevantes.';
                chunkSummaries.push(`Lote ${chunkIndex}:\n${chunkContent}`);
            } catch (chunkError) {
                console.error(`[github-models] erro no chunk ${chunkIndex}:`, chunkError.message);
                chunkSummaries.push(`Lote ${chunkIndex}: Falha ao processar.`);
            }
        }

        // Consolida os chunks
        console.log(`[github-models] consolidando ${chunkSummaries.length} chunks`);

        const finalResponse = await createChatCompletion({
            model,
            messages: [
                { role: 'system', content: SYSTEM_INSTRUCTIONS },
                { role: 'user', content: buildFinalUserPrompt({ chunkSummaries }) }
            ],
            maxTokens: 800
        });

        const finalContent = finalResponse?.choices?.[0]?.message?.content?.trim();
        console.log(`[github-models] resumo final gerado com sucesso`);

        return {
            summary: finalContent || 'N√£o foi poss√≠vel gerar o resumo no momento.'
        };

    } catch (error) {
        console.error('[github-models] erro ao gerar resumo:', error.message);
        throw error;
    }
};

module.exports = {
    summarizeLogs
};
